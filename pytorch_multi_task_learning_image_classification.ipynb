{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_multi-task-learning_image classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 이미지 다중 속성 분류 - Pytorch Ver\n",
        "\n",
        "\n",
        "> 운송수단 이미지 분류 - 타입, 색상의 큰 카테고리로 나눌 수 있음 \n",
        ">> 타입의 세부 속성(5): bike, car, helicopter, ship, truck \n",
        "\n",
        ">> 색상의 세부 속성(4): black, red, blue, green \n"
      ],
      "metadata": {
        "id": "s07YuzguL7y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDSOR2xeMZX2",
        "outputId": "e21c2010-282e-41f1-882b-a352466b2b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import tensorflow as tf \n",
        "import torch "
      ],
      "metadata": {
        "id": "0XMTpZ6GMQxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "T0zl8cdFNFWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_JUKf2yLzUQ",
        "outputId": "62f163a1-7cc5-409e-8ab0-5eb661f9fd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "#드라이브-코랩 연동 \n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "from coca_pytorch import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94SJSDm1Nhg8",
        "outputId": "d2a9e3ea-4ca4-4feb-9eb0-512de7567b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/MyDrive/dataset/automobile_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVgtBK2PMNYW",
        "outputId": "2691395c-7979-452a-9e47-a38354246cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/dataset/automobile_img\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/MyDrive/dataset/automobile_img'\n",
        "list_of_data = []\n",
        "img_size = 128\n",
        "for fol in os.listdir(data_dir):\n",
        "    for file in os.listdir(os.path.join(data_dir, fol)):\n",
        "        json_dict = {}\n",
        "        img_arr = cv2.imread(os.path.join(os.path.join(data_dir, fol), file))[...,::-1] #convert BGR to RGB format\n",
        "        resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "        json_dict['image'] = resized_arr\n",
        "        json_dict['class_1'] = fol.split('_')[0]\n",
        "        json_dict['class_2'] = fol.split('_')[1]\n",
        "        list_of_data.append(json_dict)\n"
      ],
      "metadata": {
        "id": "c9UI2yV0MN0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import lexists\n",
        "x_train, y_train = [], []\n",
        "NUM_IMG = len(list_of_data)\n",
        "\n",
        "for i in range(NUM_IMG):\n",
        "  x_train.append(list_of_data[i]['image'])\n",
        "  y_train.append([list_of_data[i]['class_1'], list_of_data[i]['class_2']])\n",
        "\n",
        "x_train = np.array(x_train)/255\n",
        "print(x_train.shape) # (948, 128, 128, 3) == (128, 128, 3)이미지 * 948개 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxxQn1xaNv9A",
        "outputId": "723dfeb7-6f5d-41db-c6e6-23f55096108d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(948, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "5GCtpl7lio10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, validation dataset 나누기 \n",
        "train_len = int(len(x_train)*0.75)\n",
        "vali_len = int(len(x_train)*0.25)\n",
        "\n",
        "x_val = x_train[train_len:]\n",
        "y_val = y_train[train_len:]\n",
        "\n",
        "x_train = x_train[:train_len]\n",
        "\n",
        "y_train = y_train[:train_len]"
      ],
      "metadata": {
        "id": "zvmWsxXUklpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train:\", train_len, \"validation:\", vali_len)"
      ],
      "metadata": {
        "id": "llK6PnNclvlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transformers as transformers\n",
        "\n",
        "from torch.utils.data import DataLoader # 배치사이즈 단위로 데이터 로딩 \n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, img_list, label_list, train_mode = True, transforms = None):\n",
        "    self.transforms = transforms\n",
        "    self.train_mode = train_mode\n",
        "    self.img_list = img_list\n",
        "    self.label_list = label_list\n",
        "\n",
        "  def __getitem__(self, index): #index번째 data를 return\n",
        "    # Get image data & transform\n",
        "    image = img_list[index]\n",
        "    if self.transforms is not None:\n",
        "        image = self.transforms(image)\n",
        "\n",
        "    if self.train_mode:\n",
        "        label = self.label_list[index]\n",
        "        return image, label\n",
        "    else:\n",
        "        return image\n",
        "\n",
        "  def __len__(self): #길이 return\n",
        "    return len(self.img_list)"
      ],
      "metadata": {
        "id": "jvKGLQGFjOfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # numpy -> PIL\n",
        "    transforms.Resize([128, 128]), #이미지 사이즈 변형\n",
        "    transforms.ToTensor(), #이미지 데이터를 tensor\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) #이미지 정규화\n",
        "\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize([128, 128]),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "                    ])"
      ],
      "metadata": {
        "id": "5mKQHBtjkhy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataset 최종 custom \n",
        "x_train = CustomDataset(x_train, y_train, train_mode = True, transforms=train_transform)\n",
        "# dataloader를 이용해 batch 단위의 데이터 만들어주기 \n",
        "train_loader = DataLoader(x_train, batch_size = 100, shuffle=True, num_workers=0)\n",
        "\n",
        "x_val = CustomDataset(x_val, y_val, train_mode=True, transforms=test_transform)\n",
        "vali_loader = DataLoader(x_val, batch_size = 100, shuffle = True, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "UVNT6O0QootT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total train imgs :',len(x_train),'/ total train batches :', len(train_loader))\n",
        "print('total valid imgs :',len(x_val), '/ total valid batches :', len(vali_loader))"
      ],
      "metadata": {
        "id": "YpYONxlMpy86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "class_1 = y_train[:, 0]\n",
        "class_2 = y_train[:, 1]\n",
        "NUM_C1 = len(set(class_1))\n",
        "NUM_C2 = len(set(class_2))\n",
        "\n",
        "map_1 = {}\n",
        "for i, j in enumerate(list(set(class_1))):\n",
        "  map_1[j] = i\n",
        "map_2 = {}\n",
        "for i, j in enumerate(list(set(class_2))):\n",
        "  map_2[j] = i\n",
        "\n",
        "print(map_1)\n",
        "print(map_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ITX_ERSN7mE",
        "outputId": "cd728e32-034b-45ed-be5c-ea985725af9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'black': 0, 'white': 1, 'green': 2, 'red': 3}\n",
            "{'bike': 0, 'ship': 1, 'truck': 2, 'helicopter': 3, 'car': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_arr1 = []\n",
        "for x in class_1:\n",
        "  class_arr1.append(map_1[x])\n",
        "class_arr1 = np.array(class_arr1)\n",
        "\n",
        "class_arr2 = []\n",
        "for x in class_2:\n",
        "  class_arr2.append(map_2[x])\n",
        "class_arr2 = np.array(class_arr2)"
      ],
      "metadata": {
        "id": "IDTnJQ1zOHlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## modeling ##\n",
        "!pip install coca-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "uE0Li58ROn7Y",
        "outputId": "048f58ee-993b-4765-fccd-010f711d4a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting coca-pytorch\n",
            "  Downloading CoCa_pytorch-0.0.6-py3-none-any.whl (6.3 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from coca-pytorch) (1.12.0+cu113)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.7/dist-packages (from coca-pytorch) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->coca-pytorch) (4.1.1)\n",
            "Installing collected packages: coca-pytorch\n",
            "Successfully installed coca-pytorch-0.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "coca_pytorch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-pytorch>=0.35.8"
      ],
      "metadata": {
        "id": "0j0C2UnKPE0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from coca_pytorch.coca_pytorch import CoCa\n",
        "import torch.optim as optim \n",
        "coca = CoCa(\n",
        "    dim = 512,                     # model dimension\n",
        "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
        "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
        "    num_tokens = 20000,            # number of text tokens\n",
        "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
        "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
        "    dim_head = 64,                 # dimension per attention head\n",
        "    heads = 8,                     # number of attention heads\n",
        "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
        "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
        ").cuda()"
      ],
      "metadata": {
        "id": "Kz4skAuiPIRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.coca = CoCa(\n",
        "    dim = 512,                     # model dimension\n",
        "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
        "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
        "    num_tokens = 20000,            # number of text tokens\n",
        "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
        "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
        "    dim_head = 64,                 # dimension per attention head\n",
        "    heads = 8,                     # number of attention heads\n",
        "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
        "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
        "    ).cuda()\n",
        "    \n",
        "    self.fc = nn.Linear( , 1024)\n",
        "    self.fc1 = nn.Linear( , NUM_C!)\n",
        "    self.fc2 = nn.Linear( , NUM_C2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.coca(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      branch1 = self.fc1(x)\n",
        "      branch1 = F.log_softmax(branch1, dim = 1)\n",
        "\n",
        "      branch2 = self.fc2(x)\n",
        "      branch2 = F.log_softmax(branch2, dim = 1)\n",
        "\n",
        "      output = [branch1, branch2]\n",
        "      return output\n"
      ],
      "metadata": {
        "id": "nuTgEmWYK3By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()"
      ],
      "metadata": {
        "id": "3YOK6kEWPqGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() # classification\n",
        "optimizer = optim.Adam(coca.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "scheduler = None"
      ],
      "metadata": {
        "id": "yGL6z8RTUKsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "def train(model, optimizer, train_loader, scheduler, device):\n",
        "  model.to(device)\n",
        "  n = len(train_loader)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss().to(device)\n",
        "  best_acc = 0\n",
        "\n",
        "  for epoch in range(1, 100):\n",
        "    ##train##\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for img, label in tqdm(iter(train_loader)): # batch 단위로 이미지 데이터 iter 반복 \n",
        "      img, label = img.to(device), label.to(device)\n",
        "      optimizer.zero_grad() # batch마다 optimizer 초기화 \n",
        "\n",
        "      logit = model(img) #예측값 산출\n",
        "      loss = criterion(logit, label) # 손실함수 계산 \n",
        "      \n",
        "      #backward propagation\n",
        "      loss.backward() # 손실함수 기준 역전파 \n",
        "      optimizer.step() # 가중치 최적화\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(\"[%d] train loss: % 10f\"%(epoch, running_loss/len(train_loader)))\n",
        "\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "    \n",
        "    ##evaluate: validation 평가##\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0 \n",
        "    with torch.no_grad(): #파라미터 업데이트 안함 \n",
        "      for img, label in tqdm(iter(val_loder)): #vali_loader : validation data set의 배치 단위 불러오기 \n",
        "        img, label = img.to(device), label.to(device)\n",
        "\n",
        "        logit = model(img)\n",
        "        val_loss += criterion(logit, label)\n",
        "        pred = logit.argmax(dim = 1, leepdim = True)\n",
        "\n",
        "        correct += pred.eq(label.view_as(pred)).sum().item() # 예측값과 label이 같으면 1, 1인 것들의 합 , view_as() : 인자로 들어가는 텐서의 모양으로 재정렬 \n",
        "    \n",
        "    val_acc = 100 * correct/len(val_loader.dataset) # 정답인 갯수/dataloader의 전체 데이터 셋의 갯수\n",
        "    print(\"val set: loss {:4f}, accuracy: {}/{} ({:0f}%) \\n\".format(val_loss / len(val_loader), correct, len(val_loader.dataset), 100 * correct / len(val_loader.dataset)))\n",
        "\n",
        "    #베스트 모델 저장 \n",
        "    if best_acc < val_acc:\n",
        "      best_acc = val_acc \n",
        "      torch.save(model,state_dict(), '/saved/best_model.pth') # 현재 디렉토리에 best_model.pth 저장 \n",
        "      \n",
        "    \n",
        "train(model, optimizer, train_loader, scheduler, device)\n"
      ],
      "metadata": {
        "id": "DaI3GPM0Ra5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "def predict(model, test_loader, device):\n",
        "  model.eval()\n",
        "  model_pred = []\n",
        "  with torch.no_grad():\n",
        "    for img in tqdm(iter(test_loader)):\n",
        "      img = img.to(device)\n",
        "\n",
        "      pred_logit = model(img)\n",
        "      pred_logit = pred_logit.argmax(dim = 1, leepdim = True).squeeze(1)\n",
        "\n",
        "      model_pred.extend(pred_logit.tolist())\n",
        "  return model_pred"
      ],
      "metadata": {
        "id": "lgQJRTahujkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "paPt5vGaXfum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}